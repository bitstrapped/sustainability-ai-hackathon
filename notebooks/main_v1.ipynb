{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    import base64\n",
    "    import vertexai\n",
    "    from vertexai.generative_models import GenerativeModel, Part\n",
    "    import vertexai.preview.generative_models as generative_models\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import json\n",
    "    import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_id = \"ds-ml-pod\"\n",
    "location = \"us-central1\"\n",
    "bucket_name = \"sustainable-ai\"\n",
    "\n",
    "\n",
    "# Paths\n",
    "train_folder = 'documents2/'\n",
    "\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "vertexai.init(project=project_id, location=location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read example ip and op pdfs from train folder\n",
    "def list_train_folder_files(bucket, train_folder):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    train_pdfs = []\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=train_folder)\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.pdf'):\n",
    "            train_pdfs.append(blob.name)\n",
    "       \n",
    "    return train_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are a Sustainable AI trained to suggesting the most CO2 efficient Google Cloud Platform technical stack based on user usecase.\\nUnderstand and use the following pieces of context to answer the question at the end. Think step-by-step and then answer. Always explain why you are answering the question the way you are.\\n',\n",
       " 'Context Input Text:  ',\n",
       " file_data {\n",
       "   mime_type: \"application/pdf\"\n",
       "   file_uri: \"gs://sustainable-ai/documents2/accelerating-climate-action-ai.pdf\"\n",
       " },\n",
       " 'Context Input Text:  ',\n",
       " file_data {\n",
       "   mime_type: \"application/pdf\"\n",
       "   file_uri: \"gs://sustainable-ai/documents2/alphabet-2023-cdp-climate-change-response.pdf\"\n",
       " },\n",
       " 'Context Input Text:  ',\n",
       " file_data {\n",
       "   mime_type: \"application/pdf\"\n",
       "   file_uri: \"gs://sustainable-ai/documents2/google-2022-climate-action-progress-update.pdf\"\n",
       " },\n",
       " 'Context Input Text:  ',\n",
       " file_data {\n",
       "   mime_type: \"application/pdf\"\n",
       "   file_uri: \"gs://sustainable-ai/documents2/google-2023-environmental-report-executive-summary.pdf\"\n",
       " },\n",
       " 'Context Input Text:  ',\n",
       " file_data {\n",
       "   mime_type: \"application/pdf\"\n",
       "   file_uri: \"gs://sustainable-ai/documents2/google-2023-supplier-responsibility-report.pdf\"\n",
       " }]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "\n",
    "# List PDF files and find corresponding JSON output files\n",
    "train_pdf_list = list_train_folder_files(bucket_name, train_folder)\n",
    "\n",
    "# Construct the prompt for each input and its corresponding output JSON\n",
    "train_prompt = []\n",
    "\n",
    "# Initial prompt setup\n",
    "train_prompt.append(\"\"\"You are a Sustainable AI trained to suggesting the most CO2 efficient Google Cloud Platform technical stack based on user usecase.\n",
    "Understand and use the following pieces of context to answer the question at the end. Think step-by-step and then answer. Always explain why you are answering the question the way you are.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Prepare train prompt\n",
    "for train_pdf in train_pdf_list:\n",
    "    train_pdf_uri = f\"gs://{bucket_name}/{train_pdf}\"\n",
    "\n",
    "\n",
    "    # Append dynamically generated content to the prompt string\n",
    "    train_prompt += [\n",
    "        \"Context Input Text:  \",\n",
    "        Part.from_uri(mime_type=\"application/pdf\", uri=train_pdf_uri)\n",
    "    ]\n",
    "train_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 1.5 model\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.4,\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}\n",
    "\n",
    "\n",
    "# LLM Model\n",
    "def generate_response(prompt):\n",
    "    responses = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=False    \n",
    "    )\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are a Sustainable AI trained to suggesting the most CO2 efficient Google Cloud Platform technical stack based on user usecase.\\nUnderstand and use the following pieces of context to answer the question at the end. Think step-by-step and then answer. Always explain why you are answering the question the way you are.\\n', 'Context Input Text:  ', file_data {\n",
      "  mime_type: \"application/pdf\"\n",
      "  file_uri: \"gs://sustainable-ai/documents2/accelerating-climate-action-ai.pdf\"\n",
      "}\n",
      ", 'Context Input Text:  ', file_data {\n",
      "  mime_type: \"application/pdf\"\n",
      "  file_uri: \"gs://sustainable-ai/documents2/alphabet-2023-cdp-climate-change-response.pdf\"\n",
      "}\n",
      ", 'Context Input Text:  ', file_data {\n",
      "  mime_type: \"application/pdf\"\n",
      "  file_uri: \"gs://sustainable-ai/documents2/google-2022-climate-action-progress-update.pdf\"\n",
      "}\n",
      ", 'Context Input Text:  ', file_data {\n",
      "  mime_type: \"application/pdf\"\n",
      "  file_uri: \"gs://sustainable-ai/documents2/google-2023-environmental-report-executive-summary.pdf\"\n",
      "}\n",
      ", 'Context Input Text:  ', file_data {\n",
      "  mime_type: \"application/pdf\"\n",
      "  file_uri: \"gs://sustainable-ai/documents2/google-2023-supplier-responsibility-report.pdf\"\n",
      "}\n",
      ", 'I want to design a PDF to JSON extractor using Gemini PRo. I want to use GCS for storage and vertex ai workbench. Suggest me the most efficient GCP solution stack to build this out']\n"
     ]
    }
   ],
   "source": [
    "test_prompt=[\"I want to design a PDF to JSON extractor using Gemini PRo. I want to use GCS for storage and vertex ai workbench. Suggest me the most efficient GCP solution stack to build this out\"]\n",
    "\n",
    "combined_prompt = train_prompt+test_prompt\n",
    "print(combined_prompt)\n",
    "\n",
    "output_response = generate_response(combined_prompt)\n",
    "output_text = output_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Sustainable AI suggests CO2-efficient GCP stack for PDF to JSON extractor:\\n\\n**Understanding the Use Case:**\\n\\n* **Functionality:** Extract text and structure from PDF documents and convert them to JSON format.\\n* **Storage:** Utilize Google Cloud Storage (GCS) for storing input PDFs and output JSON files.\\n* **Processing:** Leverage Vertex AI Workbench for development and potentially model training/inference if needed.\\n\\n**CO2 Efficiency Considerations:**\\n\\n* **Minimize resource usage:** Choose the right services and configurations to avoid unnecessary energy consumption.\\n* **Location optimization:** Select regions with lower carbon intensity for data centers and processing.\\n* **Renewable energy:** Prioritize regions and services powered by renewable energy sources.\\n\\n**Suggested GCP Stack:**\\n\\n1. **Storage:**\\n    * **GCS:** Store input PDFs and output JSON files in GCS buckets. Choose a regional or multi-regional location based on data residency and access needs. Consider using lifecycle management rules to automatically archive or delete older files to optimize storage costs and resource usage.\\n\\n2. **Processing:**\\n    * **Vertex AI Workbench:** Develop and test your PDF to JSON extractor code in a managed Jupyter Notebook environment. Choose a machine type that balances performance with energy efficiency. Consider using preemptible instances for non-critical tasks to further reduce costs and carbon footprint.\\n    * **Cloud Functions (Optional):** If you need to trigger the extraction process automatically upon uploading a PDF to GCS, consider using Cloud Functions. This serverless option ensures you only pay for the resources used during execution, minimizing idle time and energy consumption.\\n    * **Cloud Run (Optional):** If your extractor requires more complex processing or longer execution times, consider deploying it as a containerized application on Cloud Run. This serverless platform automatically scales resources based on demand, optimizing resource utilization and energy efficiency.\\n\\n3. **Additional Considerations:**\\n    * **Region Selection:** Choose regions with lower carbon intensity and higher renewable energy usage. Google provides information on the carbon footprint of its data centers in different regions.\\n    * **Machine Learning (Optional):** If your extractor benefits from machine learning for tasks like table recognition or information extraction, consider using Vertex AI services for training and deploying models. Choose efficient model architectures and training methods to minimize energy consumption.\\n    * **Monitoring and Optimization:** Continuously monitor the resource usage and carbon footprint of your application. Use tools like Cloud Monitoring and Carbon Footprint to identify optimization opportunities and further reduce your environmental impact.\\n\\n**Justification:**\\n\\n* **GCS:** Offers scalable and cost-effective storage with lifecycle management options to optimize resource usage.\\n* **Vertex AI Workbench:** Provides a managed environment for development and experimentation, allowing you to choose the right machine type for your needs.\\n* **Serverless Options:** Cloud Functions and Cloud Run minimize idle time and automatically scale resources based on demand, leading to efficient resource utilization and lower energy consumption.\\n* **Region and ML Optimization:** Choosing regions with lower carbon intensity and optimizing machine learning models further reduces the carbon footprint.\\n\\n**Additional Sustainability Tips:**\\n\\n* **Optimize code for efficiency:** Write efficient code to minimize processing time and resource usage.\\n* **Use batch processing:** Process multiple PDFs together to improve resource utilization.\\n* **Turn off resources when not in use:** Avoid unnecessary energy consumption by shutting down instances and services when not needed.\\n\\n**By following these recommendations, you can build a CO2-efficient PDF to JSON extractor on GCP while maintaining high performance and scalability.**\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roserocket_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
